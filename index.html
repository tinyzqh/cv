<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhiqiang He</title>

  <meta name="author" content="Zhiqiang He">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Zhiqiang He (何志强)
                  </p>

                  <p>
                    I hold a bachelor's degree from East China JiaoTong University and a master's from Northeastern
                    University, with a focus on Reinforcement Learning. My academic journey involved extensive research
                    under Professors <a href="https://yzw.tzc.edu.cn/info/1021/1084.htm">Pengzhan Chen</a> and <a
                      href="http://faculty.neu.edu.cn/wangjiao/zh_CN/zhym/75393/list/index.htm">Jiao Wang</a> from July
                    2017 to June 2022.

                    In my professional capacity, I interned as a Research Engineer at Baidu in Beijing, guided by
                    Xiaomin Yuan from June to September 2021. Subsequently, I served as a Reinforcement Learning
                    Algorithms Engineer at <a href="https://www.inspirai.com/">InspirAI</a> from June 2022 to May 2023,
                    where I applied my academic expertise to
                    practical AI applications.

                  </p>
                  <p style="text-align:center">
                    <a href="tinyzqh@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=v6o0Dz8AAAAJ">Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/tinyzqh/">Github</a> &nbsp;/&nbsp;
                    <a href="https://www.zhihu.com/people/zhiqianghe">Zhihu</a> &nbsp;/&nbsp;

                    <a href="zhiqianghe.blog.csdn.net">CSDN</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/tinyzqh.png"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/tinyzqh.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Experience</h2>
                  <p>

                    In the summer of 2021, I had the opportunity to intern as a Research Engineer at Baidu AI Cloud in
                    Beijing, collaborating closely with Xiaomin Yuan. During this time, we developed an innovative
                    multi-agent cooperative adversarial algorithm, which we termed Expert Data-Assisted Multi-Agent
                    Proximal Policy Optimization (EDA-MAPPO). Our work culminated in the release of a performance video
                    showcasing our algorithm, which is now available on <a
                      href="https://www.bilibili.com/video/BV1LV411p7KD/?spm_id_from=333.999.0.0&vd_source=d8ab7686ea514acb6635faa5d2227d61">Bilibili</a>
                    and the accompanying <a href="https://github.com/tinyzqh/light_mappo">source code</a>.
                  </p>

                  <p>
                    Between June 2022 and May 2023, I served as a Reinforcement Learning Algorithms Engineer at
                    InspirAI. My team and I developed and refined a versatile AI model for card games, successfully
                    deploying it in games like Hearthstone, Dou Dizhu (defeating professional players), and Guan Dan.
                    Notably, we created a Doudizhu AI, which was launched on the <a
                      href="https://www.taptap.cn/moment/356764971171842277">Taptop platform</a>.
                  </p>

                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Publication / Preprint</h2>
                </td>

              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://www.mdpi.com/1999-4893/11/5/65">
                  <span class="papertitle">Control Strategy of Speed Servo Systems Based on Deep Reinforcement
                    Learning
                  </span>
                </a>
                <br>
                <a href="https://yzw.tzc.edu.cn/info/1021/1084.htm">Pengzhan Chen</a>,
                <strong>Zhiqiang He</strong>,
                <a>Chuanxi Chen</a>,
                <a>Jiahong Xu</a>,
                <br>
                <em>Algorithms 11, no. 5: 65.<em>, 2018, <a
                      href="https://github.com/tinyzqh/control-of-jump-systems-based-on-reinforcement-learning">Code</a>
                    <br>
              </td>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322001492">
                  <span class="papertitle">Erlang planning network: An iterative model-based reinforcement learning with
                    multi-perspective
                  </span>
                </a>
                <br>
                <a href="http://faculty.neu.edu.cn/wangjiao/zh_CN/zhym/75393/list/index.htm">Jiao Wang</a>,
                <a>Lemin Zhang</a>,
                <strong>Zhiqiang He</strong>,
                <a>Can Zhu</a>,
                <a>Zihui Zhao</a>,
                <br>
                <em>Pattern Recognition, 128: 108668.<em>, 2022, <a
                      href="https://github.com/tinyzqh/Erlang-planning-network">Code</a>
                    <br>
              </td>
            </tbody>
          </table>





          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <a href="https://github.com/jonbarron/jonbarron_website">Credits</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


        </td>
      </tr>
  </table>
</body>

</html>