<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>Zhiqiang He</title>
    
    <meta name="author" content="Zhiqiang He">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
</head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                    Zhiqiang He (何志强)
                </p>
                <p>
                    I am a first-year Phd at the University of Electric Communications in Japan. I have a master's degree from <a href="https://english.neu.edu.cn/">Northeastern
                      University</a>, and my research direction is Reinforcement Learning. My academic research journey began in <a href="https://jxaco.ecjtu.edu.cn/">
                      Jiangxi Province Advanced Control and Key Optimization Laboratory</a>. July 2017 to June 2019, I worked under the
                    guidance of Professor <a href="https://yzw.tzc.edu.cn/info/1021/1084.htm">Pengzhan Chen</a>. Subsequently, from July 2019 to June 2022, I continued my research at the <a
                      href="http://www.ise.neu.edu.cn/2019/0516/c5332a125036/page.htm">
                      Deep Learning and Advanced Intelligent Decision-Making Research Institute</a> , mentored by
                    Professor <a href="http://faculty.neu.edu.cn/wangjiao/zh_CN/zhym/75393/list/index.htm">Jiao
                      Wang</a>.
                  </p>

                  <p>
                    In my professional capacity, I interned as a Research Engineer at Baidu in Beijing, from June to September 2021. Subsequently, I served as a Reinforcement Learning
                    Algorithms Engineer at <a href="https://www.inspirai.com/">InspirAI</a> from June 2022 to May 2023.

                  </p>
                <p style="text-align:center">
                    <a href="mailto:tinyzqh@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/CVZhiqiangHe.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=v6o0Dz8AAAAJ">Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/tinyzqh/">Github</a> &nbsp;/&nbsp;
                    <a href="https://www.zhihu.com/people/zhiqianghe">Zhihu</a> &nbsp;/&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/tinyzqh.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/tinyzqh.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
        </table>
          
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Experience</h2>
                  <p>
                    Between June 2022 and May 2023, I served as a Reinforcement Learning Algorithms Engineer at
                    <a href="https://www.inspirai.com/">InspirAI</a>. I put forward and optimized a general artificial intelligence modeling paradigm suitable for card games,
                    which was successfully deployed in Hearthstone, Dou Dizhu (defeated professional players), and
                    Guan Dan. Notably, The Doudizhu AI has been launched on the <a
                      href="https://www.taptap.cn/moment/356764971171842277">Taptop platform</a>.
                  </p>

                  <p>
                    In the summer of 2021, I had the opportunity to intern as a Research Engineer at Baidu AI Cloud in
                    Beijing. I  developed an innovative
                    multi-agent cooperative adversarial algorithm, which we termed Expert Data-Assisted Multi-Agent
                    Proximal Policy Optimization (EDA-MAPPO). Our work finally released a video
                    showing the performance of our algorithm, which has been published the <a href="https://github.com/tinyzqh/light_mappo">Source Code</a>. 
                    At the same time, we called "superfly" team completed a <a href="https://www.ecole.ai/2021/ml4co-competition/">machine learning for combinatorial optimization competition</a> (9/23).
                  </p>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Academic Activities</h2>
                  <p>
                    Served as a peer reviewer for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6488907">IEEE Internet of Things Journal</a>.
                  </p>
              </td>
            </tr>
          </tbody>
        </table>

    <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Publication / Preprint</h2>
            </td>

        </tr>
        </tbody>
    </table>
          
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <tr onmouseout="mspp()" onmouseover="mspp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <!-- <div class="two" id='mspp_image'><video  width=100% muted autoplay loop>
                <source src="data/mspp.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='data/mspp.jpg' width=130%, style="margin-top:22px;">
              </div>
              <script type="text/javascript">
                function mspp_start() {
                  document.getElementById('mspp_image').style.opacity = "1";
                }
      
                function mspp() {
                  document.getElementById('mspp_image').style.opacity = "0";
                }
                mspp()
              </script>
            </td>
            <td style="padding:50px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025524012751">
                <span class="papertitle">Understanding World Models through Multi-Step Pruning Policy via Reinforcement Learning</span>
              </a>
              <br>
              <strong>Zhiqiang He</strong>,
              <a>Wen Qiu</a>,
              <a>Wei Zhao</a>,
              <a>Xun Shao</a>,
              <a>Zhi Liu</a>,
              <!-- <br> -->
              <br>
              <em>Information Sciences<em>, 2024, <a href="https://github.com/tinyzqh/MSPP">Source Code</a>, (IF=8.1)
              <br>
              
              <p>
              Parallel Multi-Step Pruning Policies enhance diversity Sampling. (Analysis of convergence theory for MSPP and its PG Theorem.)
              </p>
            </td>
          </tr>
      
        <tr onmouseout="erlang()" onmouseover="erlang_start()" style="margin: 0;">
            <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
                <!-- <div class="two" id='mspp_image'><video  width=100% muted autoplay loop>
                <source src="data/mspp.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='data/erlang.jpg' width=120%, style="margin-top:22px;">
            </div>
            <script type="text/javascript">
                function erlang_start() {
                document.getElementById('erlang_image').style.opacity = "1";
                }
                function erlang() {
                document.getElementById('erlang_image').style.opacity = "0";
                }
                erlang()
            </script>
            </td>
            <td style="padding:50px;width:75%;vertical-align:middle">
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322001492">
                <span class="papertitle">Erlang planning network: An iterative model-based reinforcement learning with
                    multi-perspective</span>
            </a>
            
            <br>
            <a>Jiao Wang</a>,
            <a>Lemin Zhang</a>,
            <strong>Zhiqiang He</strong>,
            <a>Can Zhu</a>,
            <a>Zihui Zhao</a>,
            <br>
            <em>Pattern Recognition<em>, 2022, <a href="https://github.com/tinyzqh/MSPP">Source Code</a>, (IF=8.5)
            <br>
    
            
            <p>
            Bi-level reinforcement learning in Model-Based Reinforcement Learning.
            </p>
            </td>
        </tr>

        <tr onmouseout="rlpid()" onmouseover="rlpid_start()" style="margin: 0;">
            <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
                <!-- <div class="two" id='mspp_image'><video  width=100% muted autoplay loop>
                <source src="data/mspp.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='data/rlpid.jpg' width=120%, style="margin-top:22px;">
            </div>
            <script type="text/javascript">
                function rlpid_start() {
                document.getElementById('rlpid').style.opacity = "1";
                }
                function erlang() {
                document.getElementById('rlpid').style.opacity = "0";
                }
                rlpid()
            </script>
            </td>
            <td style="padding:50px;width:75%;vertical-align:middle">
            <a href="https://www.mdpi.com/1999-4893/11/5/65">
                <span class="papertitle">Control Strategy of Speed Servo Systems Based on Deep Reinforcement
                    Learning</span>
            </a>
            
            <br>
            <a href="https://yzw.tzc.edu.cn/info/1021/1084.htm">Pengzhan Chen</a>,
            <strong>Zhiqiang He</strong>,
            <a>Chuanxi Chen</a>,
            <a>Jiahong Xu</a>,

            <br>
            <em>Algorithms 11, no. 5: 65.<em>, 2018, <a href="https://github.com/tinyzqh/control-of-jump-systems-based-on-reinforcement-learning">Source Code</a>,
                    (Cited 50 times)
            <br>
    
            
            <p>
            First paper applied Reinforcement Learning in Jump Speed Servo System.
            </p>
            </td>
        </tr>




    </table>


  


            
  <table
    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            <a href="https://github.com/jonbarron/jonbarron_website">Credits</a>.
          </p>
        </td>
      </tr>
    </tbody>
  </table>



        </td>
      </tr>
    </table>
  </body>
</html>